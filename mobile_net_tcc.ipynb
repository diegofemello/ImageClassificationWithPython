{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importando as bibliotecas e Preparando os dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para o Tensorflow e Keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import img_to_array, load_img\n",
    "from keras import models, layers\n",
    "\n",
    "# Bibliotecas para o modelo de rede neural convolucional\n",
    "from keras.applications import MobileNetV3Small\n",
    "from keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "# Bibliotecas para avaliação do modelo\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarando variáveis\n",
    "\n",
    "# Caminho dos datasets\n",
    "path = './chest_xray/'\n",
    "normal_classe = 'NORMAL'\n",
    "pneumonia_classe = 'PNEUMONIA'\n",
    "\n",
    "# Treino\n",
    "treinamento_folder = os.path.join(path, 'train/')\n",
    "treinamento_normal_dir = os.path.join(treinamento_folder, normal_classe)\n",
    "treinamento_pneumonia_dir = os.path.join(treinamento_folder, pneumonia_classe)\n",
    "\n",
    "# Teste\n",
    "teste_folder = os.path.join(path, 'test/')\n",
    "teste_normal_dir = os.path.join(teste_folder, normal_classe)\n",
    "teste_pneumonia_dir = os.path.join(teste_folder, pneumonia_classe)\n",
    "\n",
    "# Validação\n",
    "validacao_folder = os.path.join(path, 'val/')\n",
    "validacao_normal_dir = os.path.join(validacao_folder, normal_classe)\n",
    "validacao_pneumonia_dir = os.path.join(validacao_folder, pneumonia_classe)\n",
    "\n",
    "# Tamanho das imagens\n",
    "img_width = 196\n",
    "img_height = 196\n",
    "\n",
    "# Variáveis para o treinamento\n",
    "batch_size = 32\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando se os nomes das classes são iguais\n",
    "\n",
    "class_names = os.listdir(treinamento_folder)\n",
    "\n",
    "if class_names == os.listdir(teste_folder) == os.listdir(validacao_folder):\n",
    "    print(\"Nome das classes: %s\" % (class_names))\n",
    "else:\n",
    "    raise Exception(\"Nome das classes não são iguais.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Found 33 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "dataset_treinamento = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    treinamento_folder,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_validacao = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    validacao_folder,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataset_teste = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    teste_folder,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando graficos de barras com a quantidade de imagens de cada classe nos datasets\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(class_names, [len(os.listdir(treinamento_normal_dir)), len(\n",
    "    os.listdir(treinamento_pneumonia_dir))])\n",
    "plt.title('DADOS DE TREINO')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(class_names, [len(os.listdir(validacao_normal_dir)),\n",
    "        len(os.listdir(validacao_pneumonia_dir))])\n",
    "plt.title('DADOS DE VALIDAÇÃO')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(class_names, [len(os.listdir(teste_normal_dir)),\n",
    "        len(os.listdir(teste_pneumonia_dir))])\n",
    "plt.title('DADOS DE TESTE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "dataset_treinamento = dataset_treinamento.map(preprocess_image)\n",
    "dataset_validacao = dataset_validacao.map(preprocess_image)\n",
    "dataset_teste = dataset_teste.map(preprocess_image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "##### Criação e treinamento do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelagem COM Transferência de Aprendizado\n",
    "# Aqui vamos preparar o modelo de acordo com nossos requisitos\n",
    "\n",
    "# Vamos preparar nossa camada de entrada para passar o tamanho da imagem. o padrão é (224,224,3). vamos alterá-lo para o tamanho da imagem que estamos usando.\n",
    "input_layer = layers.Input(shape=(img_width, img_height, 3))\n",
    "\n",
    "# Inicializando o modelo de transferência  com as propriedades apropriadas para nossas necessidades.\n",
    "# Estamos passando parâmetros como:\n",
    "# 1) weights='imagenet' - Usando isso, estamos carregando os pesos como os pesos originais.\n",
    "# 2) input_tensor para passar o modelo usando input_tensor\n",
    "# 3) Queremos alterar a última camada, então não estamos incluindo a camada superior\n",
    "\n",
    "model_tl = MobileNetV3Small(\n",
    "    weights='imagenet', input_tensor=input_layer, include_top=False)\n",
    "\n",
    "# Visualizando o sumário do modelo com nossas propriedades.\n",
    "model_tl.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando a última camada do modelo e adicionando Flatten e Dense após ela\n",
    "\n",
    "# Ultima camada do modelo\n",
    "last_layer = model_tl.output\n",
    "\n",
    "# Adicionando uma camada Flatten\n",
    "flatten = layers.Flatten()(last_layer)\n",
    "\n",
    "# Adicionando uma camada Dense para a camada de saída final\n",
    "output_layer = layers.Dense(2, activation='softmax')(flatten)\n",
    "\n",
    "# Criando o modelo com a camada de entrada e a camada de saída\n",
    "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Sumário do modelo final com as camadas adicionadas\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congelando todas as camadas, exceto a última camada para impedir que o modelo de transferência do conhecimento seja treinado novamente\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    model\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilação do modelo\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando imagens brutas apenas para revisão\n",
    "def plot_dataset(dataset):\n",
    "    plt.gcf().clear()\n",
    "    plt.figure(figsize=(10, 15))\n",
    "\n",
    "    for features, labels in dataset.take(1):\n",
    "        for i in range(8):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.imshow(features[i].numpy().astype('uint8'))\n",
    "            label = np.argmax(labels, axis=1)[i]\n",
    "\n",
    "            plt.title(class_names[label])\n",
    "\n",
    "\n",
    "plot_dataset(dataset_treinamento)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "\n",
    "from keras import callbacks\n",
    "\n",
    "# Monitora a perda de validação e interrompe o treinamento quando a perda\n",
    "# de validação não melhora por 5 épocas consecutivas para evitar overfitting\n",
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                        mode=\"min\", patience=5,\n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_treinamento,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=True,\n",
    "    validation_data=dataset_validacao,\n",
    "    callbacks=[earlystopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para plotar gráficos de acurácia e loss do modelo\n",
    "def plot_model(history):\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range = range(len(accuracy))\n",
    "\n",
    "    plt.gcf().clear()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Acurácia')\n",
    "    plt.plot(epochs_range, accuracy, label='Treinamento')\n",
    "    plt.plot(epochs_range, val_accuracy, label='Validação')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Perda')\n",
    "    plt.plot(epochs_range, loss, label='Treinamento')\n",
    "    plt.plot(epochs_range, val_loss, label='Validação')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "\n",
    "plot_model(history)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "##### Validação do modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_results(dataset):\n",
    "\n",
    "    features, labels = dataset.as_numpy_iterator().next()\n",
    "    predictions = model.predict(features)\n",
    "\n",
    "    num_rows = 4\n",
    "    num_cols = 4\n",
    "    total = num_rows*num_cols\n",
    "\n",
    "    plt.gcf().clear()\n",
    "    plt.figure(figsize=(2*num_cols, 2*num_rows))\n",
    "\n",
    "    len = predictions.shape[0]\n",
    "\n",
    "    arrayindex = random.sample(range(len), 25)\n",
    "\n",
    "    for i in range(total):\n",
    "        plt.subplot(num_rows, num_cols, i+1)\n",
    "        index = arrayindex[i]\n",
    "\n",
    "        predictions_array = predictions[index]\n",
    "        true_label = np.argmax(labels, axis=1)[index]\n",
    "\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.imshow(features[index].astype('uint8'), cmap=plt.cm.binary)\n",
    "\n",
    "        predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "        if (predicted_label == true_label):\n",
    "            color = 'green'\n",
    "        else:\n",
    "            color = 'red'\n",
    "\n",
    "        percentage = 100*np.max(predictions_array)\n",
    "\n",
    "        plt.xlabel(\"{} {:2.0f}% \\n ({})\".format(\n",
    "            class_names[predicted_label], percentage, class_names[true_label]), color=color)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(dataset_validacao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(dataset_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(dataset):\n",
    "    count = 0\n",
    "    label_list = []\n",
    "    prediction_list = []\n",
    "\n",
    "    for features, labels in dataset:\n",
    "        predictions = model.predict(features)\n",
    "        for i in range(batch_size):\n",
    "            label = np.argmax(labels[i])\n",
    "            label_list.append(label)\n",
    "\n",
    "            prediction_list.append(np.argmax(predictions[i]))\n",
    "            count += 1\n",
    "\n",
    "            if (i+1 == len(predictions)):\n",
    "                break\n",
    "\n",
    "    return label_list, prediction_list\n",
    "\n",
    "\n",
    "val_labels, val_predictions = get_results(dataset_validacao)\n",
    "test_labels, test_predictions = get_results(dataset_teste)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.suptitle(\"Matriz de confusão\")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(predictions, labels, title='Confusion Matrix', i=1):\n",
    "    plt.subplot(1, 2, i)\n",
    "\n",
    "    # Matriz de confusão\n",
    "    matrix = confusion_matrix(labels, predictions)\n",
    "\n",
    "    porcentagem_acerto = (matrix[0, 0] + matrix[1, 1]) / np.sum(matrix) * 100\n",
    "\n",
    "    # print(f'Porcentagem de acerto - {title}: {round(porcentagem_acerto)}%')\n",
    "\n",
    "    plt.imshow(matrix, cmap=plt.cm.Blues)\n",
    "\n",
    "    # Adicionando os nomes das classes e os valores\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = 'ACERTO' if i == j else 'ERRO'\n",
    "            text_final = f'{matrix[i, j]}\\n{text} ({class_names[i]})'\n",
    "            plt.text(i, j, text_final, va='center', ha='center')\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(val_labels, val_predictions,\n",
    "                      title='Validação', i=1)\n",
    "plot_confusion_matrix(test_labels, test_predictions,\n",
    "                      title='Teste', i=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de precisão, recall e f1-score\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.suptitle(\"Precisão, Recall e F1-Score\")\n",
    "\n",
    "\n",
    "def plot_precision(predictions, labels, title='Precisão', i=1):\n",
    "    precision = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro')\n",
    "\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.bar(['Precisão'], precision[0], color='blue')\n",
    "    plt.bar(['Recall'], precision[1], color='red')\n",
    "    plt.bar(['F1-Score'], precision[2], color='green')\n",
    "\n",
    "    # Exibindo os valores de precisão, recall e f1-score\n",
    "    plt.text(0, precision[0], round(precision[0], 2), ha='center', va='bottom')\n",
    "    plt.text(1, precision[1], round(precision[1], 2), ha='center', va='bottom')\n",
    "    plt.text(2, precision[2], round(precision[2], 2), ha='center', va='bottom')\n",
    "    plt.title(title)\n",
    "    return precision\n",
    "\n",
    "\n",
    "plot_precision(val_predictions, val_labels,\n",
    "               title='Validação', i=1)\n",
    "plot_precision(test_predictions, test_labels,\n",
    "               title='Teste', i=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c397320c509d2f3643371b5baf055b56cff1442497b1bc23824b4a0b318cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
